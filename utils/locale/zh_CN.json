{
    "Input audio files to inference": "输入音频推理",
    "Input folder to inference": "输入文件夹推理",
    "Please upload at least one audio file!": "请至少上传一个音频文件！",
    "Please download the pretrained model xlsr_53_56k.pt from https://dl.fbaipublicfiles.com/fairseq/wav2vec/xlsr_53_56k.pt and put it in the 'pretrained' folder!": "请从点击链接 https://dl.fbaipublicfiles.com/fairseq/wav2vec/xlsr_53_56k.pt) 下载预训练模型 xlsr_53_56k.pt，并将其放入 'pretrained' 文件夹！",
    "Please select a model!": "请选择一个模型！",
    "Please select an output folder!": "请选择一个输出文件夹！",
    "Please select an input folder!": "请选择一个输入文件夹！",
    "Inference process finished.": "推理完成",
    "Inference process stopped by user.": "用户强制停止",
    "No active inference process.": "没有正在运行的推理过程",
    "Medley Vox is a [dataset for testing algorithms for separating multiple singers](https://arxiv.org/pdf/2211.07302) within a single music track. Also, the [authors of Medley Vox](https://github.com/jeonchangbin49/MedleyVox) proposed a neural network architecture for separating singers. However, unfortunately, they did not publish the weights. Later, their training process was [repeated by Cyru5](https://huggingface.co/Cyru5/MedleyVox/tree/main), who trained several models and published the weights in the public domain. Now this WebUI is created to use the trained models and weights for inference. Here are some precautions:<br>1. Put the [downloaded models](https://huggingface.co/Cyru5/MedleyVox) in the 'checkpoints' folder in folder format, with each model folder containing a model file (.pth) and its corresponding configuration file (.json).<br>2. If you use overlapadd and the choice of model is 'w2v' or 'w2v_chunk', you need to download the pretrained model [xlsr_53_56k.pt](https://dl.fbaipublicfiles.com/fairseq/wav2vec/xlsr_53_56k.pt) and put it in the 'pretrained' folder.<br>3. At present, the audio output sampling rate supported by the model is 24000kHz and cannot be changed. To solve this, you can use [AudioSR](https://github.com/haoheliu/versatile_audio_super_resolution), [Apollo](https://github.com/JusperLee/Apollo), or [Music Source Separation Training](https://github.com/ZFTurbo/Music-Source-Separation-Training) for audio super-resolution.<br>4. When using WebUI on cloud platforms or Colab, please place the audio to be processed in the 'inputs' folder, and the processing results will be stored in the 'results' folder. The 'Select folder' and 'Open folder' buttons are invalid in the cloud.": "Medley Vox 是一个[用于测试多歌手分离算法的数据集](https://arxiv.org/pdf/2211.07302)，适用于单个音乐轨道。此外，[Medley Vox 的作者](https://github.com/jeonchangbin49/MedleyVox) 提出了一个用于分离歌手的神经网络架构。但遗憾的是，他们没有公开模型权重。后来，[Cyru5](https://huggingface.co/Cyru5/MedleyVox/tree/main) 复现了他们的训练过程，并训练了多个模型，将权重发布到了公共领域。这个 WebUI 用于加载这些训练模型并进行推理。以下是一些注意事项：<br>1. 请将[下载的模型](https://huggingface.co/Cyru5/MedleyVox) 放入 'checkpoints' 文件夹，每个模型文件夹应包含模型文件 (.pth) 及其对应的配置文件 (.json)。<br>2. 如果使用 overlapadd 且选择的选项为 'w2v' 或 'w2v_chunk'，则需要下载预训练模型 [xlsr_53_56k.pt](https://dl.fbaipublicfiles.com/fairseq/wav2vec/xlsr_53_56k.pt) 并将其放入 'pretrained' 文件夹。<br>3. 目前，模型支持的音频输出采样率为 24000kHz，无法更改。若需调整，可使用 [AudioSR](https://github.com/haoheliu/versatile_audio_super_resolution)、[Apollo](https://github.com/JusperLee/Apollo) 或 [Music Source Separation Training](https://github.com/ZFTurbo/Music-Source-Separation-Training) 进行音频超分处理。<br>4. 在云平台或 Colab 上使用 WebUI 时，请将待处理的音频放入 'inputs' 文件夹，处理结果将存入 'results' 文件夹。在云端环境下，“选择文件夹”和“打开文件夹”按钮无效。",
    "Common options": "常规设置",
    "Select Model": "选择模型",
    "Select which model you want to use.": "选择您要使用的模型。",
    "Use overlapadd": "使用 overlapadd",
    "Use overlapadd functions, ola, ola_norm, w2v will work with ola_window_len, ola_hop_len argugments. w2v_chunk and sf_chunk is chunk-wise processing based on VAD, so you have to specify the vad_method args. If you use sf_chunk (spectral_featrues_chunk), you also need to specify spectral_features.<br>If the input is too long, it may be impossible to inference due to lack of VRAM. In that case, use 'use_overlapadd'. Among the 'use_overlapadd' options, 'ola', 'ola_norm', and 'w2v' all work well. Use w2v_chunk or sf_chunk if these fail or as desired. You can also try experimenting with 'vad_method' options spec and webrtc when using either of the '_chunk' methods. Chunking has proven to be very useful therefore it is on by default.": "使用 overlapadd 函数，ola、ola_norm、w2v 将与 ola_window_len、ola_hop_len 参数一起工作。w2v_chunk 和 sf_chunk 是基于 VAD 的分块处理，因此需要指定 vad_method 参数。如果使用 sf_chunk（spectral_featrues_chunk），还需要指定 spectral_features。<br>如果输入音频过长，可能会因 VRAM 不足导致无法推理。在这种情况下，请务必使用此选项。在此选项中，'ola'、'ola_norm' 和 'w2v' 都效果良好。也可以选择 w2v_chunk 或 sf_chunk。使用 '_chunk' 方法时，还需要设置 'vad_method' 中的 spec 和 webrtc 选项。分块方法已证明非常有效，因此默认启用。",
    "Use GPU": "使用 GPU",
    "Use GPU for inference.": "使用 GPU 进行推理。",
    "Save results in separate folders": "将处理结果以单独文件夹形式保存",
    "Save results in separate folders with the same name as the input file.": "将结果保存在与输入音频同名的单独文件夹中。",
    "Skip error files": "跳过处理错误的文件",
    "Skip error files while separating instead of stopping.": "在分离时跳过处理错误错误的文件而不是停止。",
    "Output format": "输出格式",
    "Select the output format.": "选择输出格式。",
    "[Click to expand] Advanced options": "[点击展开] 高级选项",
    "VAD method": "VAD 方法",
    "What method do you want to use for 'voice activity detection (vad) -- split chunks -- processing. Only valid when 'w2v_chunk' or 'sf_chunk' for args.use_overlapadd.": "使用哪种方法进行“语音活动检测（VAD）分块处理”。仅当 use_overlapadd选择 'w2v_chunk' 或 'sf_chunk' 参数时有效。",
    "Spectral features": "频谱特征",
    "What spectral feature do you want to use in correlation calc in speaker assignment (only valid when using sf_chunk)": "在相关性计算中，您希望使用哪种频谱特征（仅在使用 sf_chunk 时有效）",
    "OLA window length": "OLA 窗口长度",
    "OLA window size in [sec], only valid when using ola or ola_norm. Set 0 to use the default value (None).": "OLA 窗口大小（秒），仅在使用 ola 或 ola_norm 时有效。设置为 0 使用默认值（None）。",
    "OLA hop length": "OLA 步长",
    "OLA hop size in [sec], only valid when using ola or ola_norm. Set 0 to use the default value (None).": "OLA 步长大小（秒），仅在使用 ola 或 ola_norm 时有效。设置为 0 使用默认值（None）。",
    "Wav2Vec nth layer output": "Wav2Vec 第 n 层输出",
    "Wav2Vec nth layer output, only valid when using w2v or w2v_chunk. For example: 0 1 2 3, default: 0": "Wav2Vec 第 n 层输出，仅在使用 w2v 或 w2v_chunk 时有效。例如：0 1 2 3，默认值：0。",
    "Use EMA model": "使用 EMA 模型",
    "Use EMA model or online model? Only vaind when args.ema it True (model trained with EMA).": "使用 EMA 模型或在线模型。仅当 args.ema 为 True 时有效（使用 EMA 训练的模型）。",
    "Mix consistent output": "混合一致输出",
    "Only valid when the model is trained with mixture_consistency loss.": "仅在模型使用 mixture_consistency 损失训练时有效。",
    "Reorder chunks": "重新排序分块",
    "OLA reorder chunks. Only valid when using ola or ola_norm.": "OLA 重新排序分块，仅在使用 ola 或 ola_norm 时有效。",
    "Input audio files": "输入音频文件",
    "Input one or more audio files": "输入一个或多个音频文件",
    "Input folder path": "输入文件夹路径",
    "Audio files in the folder will be used for inference.": "文件夹中的音频文件将用于推理。",
    "Select folder": "选择文件夹",
    "Open folder": "打开文件夹",
    "Output folder path": "输出文件夹路径",
    "Audio files will be saved in this folder.": "处理后的音频文件将保存在此文件夹中。",
    "Output message": "输出信息",
    "Forced stop inference": "强制停止推理"
}