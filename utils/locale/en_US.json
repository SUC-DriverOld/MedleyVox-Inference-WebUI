{
    "Input audio files to inference": "Input audio files to inference",
    "Input folder to inference": "Input folder to inference",
    "Please upload at least one audio file!": "Please upload at least one audio file!",
    "Please download the pretrained model xlsr_53_56k.pt from https://dl.fbaipublicfiles.com/fairseq/wav2vec/xlsr_53_56k.pt and put it in the 'pretrained' folder!": "Please download the pretrained model xlsr_53_56k.pt from https://dl.fbaipublicfiles.com/fairseq/wav2vec/xlsr_53_56k.pt and put it in the 'pretrained' folder!",
    "Please select a model!": "Please select a model!",
    "Please select an output folder!": "Please select an output folder!",
    "Please select an input folder!": "Please select an input folder!",
    "Inference process finished.": "Inference process finished.",
    "Inference process stopped by user.": "Inference process stopped by user.",
    "No active inference process.": "No active inference process.",
    "Medley Vox is a [dataset for testing algorithms for separating multiple singers](https://arxiv.org/pdf/2211.07302) within a single music track. Also, the [authors of Medley Vox](https://github.com/jeonchangbin49/MedleyVox) proposed a neural network architecture for separating singers. However, unfortunately, they did not publish the weights. Later, their training process was [repeated by Cyru5](https://huggingface.co/Cyru5/MedleyVox/tree/main), who trained several models and published the weights in the public domain. Now this WebUI is created to use the trained models and weights for inference. Here are some precautions:<br>1. Put the [downloaded models](https://huggingface.co/Cyru5/MedleyVox) in the 'checkpoints' folder in folder format, with each model folder containing a model file (.pth) and its corresponding configuration file (.json).<br>2. If you use overlapadd and the choice of model is 'w2v' or 'w2v_chunk', you need to download the pretrained model [xlsr_53_56k.pt](https://dl.fbaipublicfiles.com/fairseq/wav2vec/xlsr_53_56k.pt) and put it in the 'pretrained' folder.<br>3. At present, the audio output sampling rate supported by the model is 24000kHz and cannot be changed. To solve this, you can use [AudioSR](https://github.com/haoheliu/versatile_audio_super_resolution), [Apollo](https://github.com/JusperLee/Apollo), or [Music Source Separation Training](https://github.com/ZFTurbo/Music-Source-Separation-Training) for audio super-resolution.<br>4. When using WebUI on cloud platforms or Colab, please place the audio to be processed in the 'inputs' folder, and the processing results will be stored in the 'results' folder. The 'Select folder' and 'Open folder' buttons are invalid in the cloud.": "Medley Vox is a [dataset for testing algorithms for separating multiple singers](https://arxiv.org/pdf/2211.07302) within a single music track. Also, the [authors of Medley Vox](https://github.com/jeonchangbin49/MedleyVox) proposed a neural network architecture for separating singers. However, unfortunately, they did not publish the weights. Later, their training process was [repeated by Cyru5](https://huggingface.co/Cyru5/MedleyVox/tree/main), who trained several models and published the weights in the public domain. Now this WebUI is created to use the trained models and weights for inference. Here are some precautions:<br>1. Put the [downloaded models](https://huggingface.co/Cyru5/MedleyVox) in the 'checkpoints' folder in folder format, with each model folder containing a model file (.pth) and its corresponding configuration file (.json).<br>2. If you use overlapadd and the choice of model is 'w2v' or 'w2v_chunk', you need to download the pretrained model [xlsr_53_56k.pt](https://dl.fbaipublicfiles.com/fairseq/wav2vec/xlsr_53_56k.pt) and put it in the 'pretrained' folder.<br>3. At present, the audio output sampling rate supported by the model is 24000kHz and cannot be changed. To solve this, you can use [AudioSR](https://github.com/haoheliu/versatile_audio_super_resolution), [Apollo](https://github.com/JusperLee/Apollo), or [Music Source Separation Training](https://github.com/ZFTurbo/Music-Source-Separation-Training) for audio super-resolution.<br>4. When using WebUI on cloud platforms or Colab, please place the audio to be processed in the 'inputs' folder, and the processing results will be stored in the 'results' folder. The 'Select folder' and 'Open folder' buttons are invalid in the cloud.",
    "Common options": "Common options",
    "Select Model": "Select Model",
    "Select which model you want to use.": "Select which model you want to use.",
    "Use overlapadd": "Use overlapadd",
    "Use overlapadd functions, ola, ola_norm, w2v will work with ola_window_len, ola_hop_len argugments. w2v_chunk and sf_chunk is chunk-wise processing based on VAD, so you have to specify the vad_method args. If you use sf_chunk (spectral_featrues_chunk), you also need to specify spectral_features.<br>If the input is too long, it may be impossible to inference due to lack of VRAM. In that case, use 'use_overlapadd'. Among the 'use_overlapadd' options, 'ola', 'ola_norm', and 'w2v' all work well. Use w2v_chunk or sf_chunk if these fail or as desired. You can also try experimenting with 'vad_method' options spec and webrtc when using either of the '_chunk' methods. Chunking has proven to be very useful therefore it is on by default.": "Use overlapadd functions, ola, ola_norm, w2v will work with ola_window_len, ola_hop_len argugments. w2v_chunk and sf_chunk is chunk-wise processing based on VAD, so you have to specify the vad_method args. If you use sf_chunk (spectral_featrues_chunk), you also need to specify spectral_features.<br>If the input is too long, it may be impossible to inference due to lack of VRAM. In that case, use 'use_overlapadd'. Among the 'use_overlapadd' options, 'ola', 'ola_norm', and 'w2v' all work well. Use w2v_chunk or sf_chunk if these fail or as desired. You can also try experimenting with 'vad_method' options spec and webrtc when using either of the '_chunk' methods. Chunking has proven to be very useful therefore it is on by default.",
    "Use GPU": "Use GPU",
    "Use GPU for inference.": "Use GPU for inference.",
    "Save results in separate folders": "Save results in separate folders",
    "Save results in separate folders with the same name as the input file.": "Save results in separate folders with the same name as the input file.",
    "Skip error files": "Skip error files",
    "Skip error files while separating instead of stopping.": "Skip error files while separating instead of stopping.",
    "Output format": "Output format",
    "Select the output format.": "Select the output format.",
    "[Click to expand] Advanced options": "[Click to expand] Advanced options",
    "VAD method": "VAD method",
    "What method do you want to use for 'voice activity detection (vad) -- split chunks -- processing. Only valid when 'w2v_chunk' or 'sf_chunk' for args.use_overlapadd.": "What method do you want to use for 'voice activity detection (vad) -- split chunks -- processing. Only valid when 'w2v_chunk' or 'sf_chunk' for args.use_overlapadd.",
    "Spectral features": "Spectral features",
    "What spectral feature do you want to use in correlation calc in speaker assignment (only valid when using sf_chunk)": "What spectral feature do you want to use in correlation calc in speaker assignment (only valid when using sf_chunk)",
    "OLA window length": "OLA window length",
    "OLA window size in [sec], only valid when using ola or ola_norm. Set 0 to use the default value (None).": "OLA window size in [sec], only valid when using ola or ola_norm. Set 0 to use the default value (None).",
    "OLA hop length": "OLA hop length",
    "OLA hop size in [sec], only valid when using ola or ola_norm. Set 0 to use the default value (None).": "OLA hop size in [sec], only valid when using ola or ola_norm. Set 0 to use the default value (None).",
    "Wav2Vec nth layer output": "Wav2Vec nth layer output",
    "Wav2Vec nth layer output, only valid when using w2v or w2v_chunk. For example: 0 1 2 3, default: 0": "Wav2Vec nth layer output, only valid when using w2v or w2v_chunk. For example: 0 1 2 3, default: 0",
    "Use EMA model": "Use EMA model",
    "Use EMA model or online model? Only vaind when args.ema it True (model trained with EMA).": "Use EMA model or online model? Only vaind when args.ema it True (model trained with EMA).",
    "Mix consistent output": "Mix consistent output",
    "Only valid when the model is trained with mixture_consistency loss.": "Only valid when the model is trained with mixture_consistency loss.",
    "Reorder chunks": "Reorder chunks",
    "OLA reorder chunks. Only valid when using ola or ola_norm.": "OLA reorder chunks. Only valid when using ola or ola_norm.",
    "Input audio files": "Input audio files",
    "Input one or more audio files": "Input one or more audio files",
    "Input folder path": "Input folder path",
    "Audio files in the folder will be used for inference.": "Audio files in the folder will be used for inference.",
    "Select folder": "Select folder",
    "Open folder": "Open folder",
    "Output folder path": "Output folder path",
    "Audio files will be saved in this folder.": "Audio files will be saved in this folder.",
    "Output message": "Output message",
    "Forced stop inference": "Forced stop inference"
}